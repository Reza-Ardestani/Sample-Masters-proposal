@misc{cao2017faithful,
      title={Faithful to the Original: Fact Aware Neural Abstractive Summarization}, 
      author={Ziqiang Cao and Furu Wei and Wenjie Li and Sujian Li},
      year={2017},
      eprint={1711.04434},
      archivePrefix={arXiv},
      primaryClass={cs.IR}
}
@InProceedings{Hou2018,
author="Hou, Liwei
and Hu, Po
and Bei, Chao",
editor="Huang, Xuanjing
and Jiang, Jing
and Zhao, Dongyan
and Feng, Yansong
and Hong, Yu",
title="Abstractive Document Summarization via Neural Model with Joint Attention",
booktitle="Natural Language Processing and Chinese Computing",
year="2018",
publisher="Springer International Publishing",
address="Cham",
pages="329--338",
abstract="",
isbn="978-3-319-73618-1"
}



@misc{vaswani2017,
  doi = {10.48550/ARXIV.1706.03762},
  
  url = {https://arxiv.org/abs/1706.03762},
  
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
  
  keywords = {Computation and Language (cs.CL), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Attention Is All You Need},
  
  publisher = {arXiv},
  
  year = {2017},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}



@Article{Popel2020,
author={Popel, Martin
and Tomkova, Marketa
and Tomek, Jakub
and Kaiser, {\L}ukasz
and Uszkoreit, Jakob
and Bojar, Ond{\v{r}}ej
and {\v{Z}}abokrtsk{\'y}, Zden{\v{e}}k},
title={Transforming machine translation: a deep learning system reaches news translation quality comparable to human professionals},
journal={Nature Communications},
year={2020},
month={Sep},
day={01},
volume={11},
number={1},
pages={4381},
abstract={The quality of human translation was long thought to be unattainable for computer translation systems. In this study, we present a deep-learning system, CUBBITT, which challenges this view. In a context-aware blind evaluation by human judges, CUBBITT significantly outperformed professional-agency English-to-Czech news translation in preserving text meaning (translation adequacy). While human translation is still rated as more fluent, CUBBITT is shown to be substantially more fluent than previous state-of-the-art systems. Moreover, most participants of a Translation Turing test struggle to distinguish CUBBITT translations from human translations. This work approaches the quality of human translation and even surpasses it in adequacy in certain circumstances.This suggests that deep learning may have the potential to replace humans in applications where conservation of meaning is the primary aim.},
issn={2041-1723},
doi={10.1038/s41467-020-18073-9},
url={https://doi.org/10.1038/s41467-020-18073-9}
}








@inproceedings{FAsum2021,
    title = "Enhancing Factual Consistency of Abstractive Summarization",
    author = "Zhu, Chenguang  and
      Hinthorn, William  and
      Xu, Ruochen  and
      Zeng, Qingkai  and
      Zeng, Michael  and
      Huang, Xuedong  and
      Jiang, Meng",
    booktitle = "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.naacl-main.58",
    doi = "10.18653/v1/2021.naacl-main.58",
    pages = "718--733",
    abstract = "Automatic abstractive summaries are found to often distort or fabricate facts in the article. This inconsistency between summary and original text has seriously impacted its applicability. We propose a fact-aware summarization model FASum to extract and integrate factual relations into the summary generation process via graph attention. We then design a factual corrector model FC to automatically correct factual errors from summaries generated by existing systems. Empirical results show that the fact-aware summarization can produce abstractive summaries with higher factual consistency compared with existing systems, and the correction model improves the factual consistency of given summaries via modifying only a few keywords.",
}

@inproceedings{falke-etal-2019-ranking,
    title = "Ranking Generated Summaries by Correctness: An Interesting but Challenging Application for Natural Language Inference",
    author = "Falke, Tobias  and
      Ribeiro, Leonardo F. R.  and
      Utama, Prasetya Ajie  and
      Dagan, Ido  and
      Gurevych, Iryna",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P19-1213",
    doi = "10.18653/v1/P19-1213",
    pages = "2214--2220",
    abstract = "While recent progress on abstractive summarization has led to remarkably fluent summaries, factual errors in generated summaries still severely limit their use in practice. In this paper, we evaluate summaries produced by state-of-the-art models via crowdsourcing and show that such errors occur frequently, in particular with more abstractive models. We study whether textual entailment predictions can be used to detect such errors and if they can be reduced by reranking alternative predicted summaries. That leads to an interesting downstream application for entailment models. In our experiments, we find that out-of-the-box entailment models trained on NLI datasets do not yet offer the desired performance for the downstream task and we therefore release our annotations as additional test data for future extrinsic evaluations of NLI.",
}



@inproceedings{SumPubMed,
  booktitle = "Proceedings of the 2021 Conference of the Association for Computational Linguistics: Student Research Workshop",
  title={SUMPUBMED: Summarization Dataset of PubMed Scientific Article},
  author={Gupta, Vivek and Bharti, Prerna and Nokhiz, Pegah and Karnick, Harish},
  publisher = "Association for Computational Linguistics",
  year = "2021",
  url={https://vgupta123.github.io/docs/121_paper.pdf}
}


@article{CNNDM,
  author    = {Ramesh Nallapati and
               Bing Xiang and
               Bowen Zhou},
  title     = {Sequence-to-Sequence RNNs for Text Summarization},
  journal   = {CoRR},
  volume    = {abs/1602.06023},
  year      = {2016},
  url       = {http://arxiv.org/abs/1602.06023},
  eprinttype = {arXiv},
  eprint    = {1602.06023},
  timestamp = {Mon, 13 Aug 2018 16:46:52 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/NallapatiXZ16.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}



@inproceedings{PubMed,
    title = "A Discourse-Aware Attention Model for Abstractive Summarization of Long Documents",
    author = "Cohan, Arman  and
      Dernoncourt, Franck  and
      Kim, Doo Soon  and
      Bui, Trung  and
      Kim, Seokhwan  and
      Chang, Walter  and
      Goharian, Nazli",
    booktitle = "Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers)",
    month = jun,
    year = "2018",
    address = "New Orleans, Louisiana",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N18-2097",
    doi = "10.18653/v1/N18-2097",
    pages = "615--621",
    abstract = "Neural abstractive summarization models have led to promising results in summarizing relatively short documents. We propose the first model for abstractive summarization of single, longer-form documents (e.g., research papers). Our approach consists of a new hierarchical encoder that models the discourse structure of a document, and an attentive discourse-aware decoder to generate the summary. Empirical results on two large-scale datasets of scientific papers show that our model significantly outperforms state-of-the-art models.",
}



@article{wafaa,
title = {Automatic text summarization: A comprehensive survey},
journal = {Expert Systems with Applications},
volume = {165},
pages = {113679},
year = {2021},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2020.113679},
url = {https://www.sciencedirect.com/science/article/pii/S0957417420305030},
author = {Wafaa S. El-Kassas and Cherif R. Salama and Ahmed A. Rafea and Hoda K. Mohamed},
keywords = {Automatic text summarization, Text summarization approaches, Text summarization techniques, Text summarization evaluation},
abstract = {.}
}


@misc{knowledge-aware,
  doi = {10.48550/ARXIV.2204.11190},
  
  url = {https://arxiv.org/abs/2204.11190},
  
  author = {Qu, Yutong and Zhang, Wei Emma and Yang, Jian and Wu, Lingfei and Wu, Jia},
  
  keywords = {Computation and Language (cs.CL), Artificial Intelligence (cs.AI), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Knowledge-aware Document Summarization: A Survey of Knowledge, Embedding Methods and Architectures},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {Creative Commons Attribution 4.0 International}
}

@misc{pgen,
  doi = {10.48550/ARXIV.1704.04368},
  
  url = {https://arxiv.org/abs/1704.04368},
  
  author = {See, Abigail and Liu, Peter J. and Manning, Christopher D.},
  
  keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Get To The Point: Summarization with Pointer-Generator Networks},
  
  publisher = {arXiv},
  
  year = {2017},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}



@inproceedings{fischer2022measuring,
  title={Measuring Faithfulness of Abstractive Summaries},
  author={Fischer, Tim and Remus, Steffen and Biemann, Chris},
  booktitle={Proceedings of the 18th Conference on Natural Language Processing (KONVENS 2022)},
  pages={63--73},
  year={2022}
}


@misc{bertScore,
  doi = {10.48550/ARXIV.1904.09675},
  
  url = {https://arxiv.org/abs/1904.09675},
  
  author = {Zhang, Tianyi and Kishore, Varsha and Wu, Felix and Weinberger, Kilian Q. and Artzi, Yoav},
  
  keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {BERTScore: Evaluating Text Generation with BERT},
  
  publisher = {arXiv},
  
  year = {2019},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@misc{bartScore,
  doi = {10.48550/ARXIV.2106.11520},
  
  url = {https://arxiv.org/abs/2106.11520},
  
  author = {Yuan, Weizhe and Neubig, Graham and Liu, Pengfei},
  
  keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {BARTScore: Evaluating Generated Text as Text Generation},
  
  publisher = {arXiv},
  
  year = {2021},
  
  copyright = {Creative Commons Zero v1.0 Universal}
}


@inproceedings{bleu,
    title = "{B}leu: a Method for Automatic Evaluation of Machine Translation",
    author = "Papineni, Kishore  and
      Roukos, Salim  and
      Ward, Todd  and
      Zhu, Wei-Jing",
    booktitle = "Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2002",
    address = "Philadelphia, Pennsylvania, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P02-1040",
    doi = "10.3115/1073083.1073135",
    pages = "311--318",
}



@inproceedings{gencomparesum,
    title = "{G}en{C}ompare{S}um: a hybrid unsupervised summarization method using salience",
    author = "Bishop, Jennifer  and
      Xie, Qianqian  and
      Ananiadou, Sophia",
    booktitle = "Proceedings of the 21st Workshop on Biomedical Language Processing",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.bionlp-1.22",
    doi = "10.18653/v1/2022.bionlp-1.22",
    pages = "220--240",
    abstract = "Text summarization (TS) is an important NLP task. Pre-trained Language Models (PLMs) have been used to improve the performance of TS. However, PLMs are limited by their need of labelled training data and by their attention mechanism, which often makes them unsuitable for use on long documents. To this end, we propose a hybrid, unsupervised, abstractive-extractive approach, in which we walk through a document, generating salient textual fragments representing its key points. We then select the most important sentences of the document by choosing the most similar sentences to the generated texts, calculated using BERTScore. We evaluate the efficacy of generating and using salient textual fragments to guide extractive summarization on documents from the biomedical and general scientific domains. We compare the performance between long and short documents using different generative text models, which are finetuned to generate relevant queries or document titles. We show that our hybrid approach out-performs existing unsupervised methods, as well as state-of-the-art supervised methods, despite not needing a vast amount of labelled training data.",
}



@inproceedings{hierarchical-encoder,
author = {Yang, Hang and Chen, Yubo and Liu, Kang and Zhao, Jun and Wang, Taifeng},
title = {Multi-Sentence Argument Linking via An Event-Aware Hierarchical Encoder},
year = {2021},
isbn = {9781450384469},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3459637.3482148},
doi = {10.1145/3459637.3482148},
abstract = {Multi-sentence argument linking aims at detecting implicit event arguments across sentences, which is indispensable when textual events span across multiple sentences in a document. Previous studies suffer from the inherent limitations of error propagation and lack the explicit modeling of the local and non-local interactions in a textual event. In this paper, we propose an event-aware hierarchical encoder for multi-sentence argument linking. Specifically, we introduce a hierarchical encoder to explicitly capture the local and global interactions in a textual event. Furthermore, we introduce an auxiliary task to predict the event-relevant context in a manner of multi-task learning, which can implicitly benefit the argument linking model to be aware of the event-relevant context. The empirical results on the widely used argument linking dataset show that our model significantly outperforms the baselines, which demonstrates the effectiveness of our proposed method.},
booktitle = {Proceedings of the 30th ACM International Conference on Information amp; Knowledge Management},
pages = {3578–3582},
numpages = {5},
keywords = {document representation, argument linking, multi-task learning},
location = {Virtual Event, Queensland, Australia},
series = {CIKM '21}
}



@misc{entity-aware-factual-summarizer,
  doi = {10.48550/ARXIV.2203.15959},
  
  url = {https://arxiv.org/abs/2203.15959},
  
  author = {Alambo, Amanuel and Banerjee, Tanvi and Thirunarayan, Krishnaprasad and Raymer, Michael},
  
  keywords = {Computation and Language (cs.CL), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Entity-driven Fact-aware Abstractive Summarization of Biomedical Literature},

}



@article{Kumar2022,
title = {An exploratory study of automatic text summarization in biomedical and healthcare domain},
journal = {Healthcare Analytics},
volume = {2},
pages = {100058},
year = {2022},
issn = {2772-4425},
doi = {https://doi.org/10.1016/j.health.2022.100058},
url = {https://www.sciencedirect.com/science/article/pii/S2772442522000223},
author = {Mukesh Kumar Rohil and Varun Magotra},
keywords = {Artificial Intelligence, Electronic Health Records, Automatic Text Summarization, Natural Language Processing, Unified Medical Language System, Medical Subject Headings},
abstract = {In the last two decades, the uses of automatic text summarization have been realized in a wide range of applications in various fields cutting across a number of verticals. Amongst these, one of the most inquired is the domain of healthcare and medicine. Many of the studies have revealed that the use of automatic text summarization in the biomedical and healthcare domain helps researchers and medical professionals save their time and access more information in considerably short spans of time. This article reports some of the recent studies that enumerate the benefits and limitations of the uses of automatic text summarization in the biomedical and healthcare domain. In addition, the paper also explores certain new possible applications of automatic text summarization in the biomedical and healthcare domain. Furthermore, it discusses the trends and vision towards future opportunities for possible research in automatic text summarization in the context of medical and healthcare domain.}
}






@inproceedings{abdullah-chali-2020-towards,
    title = "Towards Generating Query to Perform Query Focused Abstractive Summarization using Pre-trained Model",
    author = "Abdullah, Deen Mohammad  and
      Chali, Yllias",
    booktitle = "Proceedings of the 13th International Conference on Natural Language Generation",
    month = dec,
    year = "2020",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.inlg-1.11",
    pages = "80--85",
    abstract = "Query Focused Abstractive Summarization (QFAS) represents an abstractive summary from the source document based on a given query. To measure the performance of abstractive summarization tasks, different datasets have been broadly used. However, for QFAS tasks, only a limited number of datasets have been used, which are comparatively small and provide single sentence summaries. This paper presents a query generation approach, where we considered most similar words between documents and summaries for generating queries. By implementing our query generation approach, we prepared two relatively large datasets, namely CNN/DailyMail and Newsroom which contain multiple sentence summaries and can be used for future QFAS tasks. We also implemented a pre-processing approach to perform QFAS tasks using a pretrained language model, BERTSUM. In our pre-processing approach, we sorted the sentences of the documents from the most query-related sentences to the less query-related sentences. Then, we fine-tuned the BERTSUM model for generating the abstractive summaries. We also experimented on one of the largely used datasets, Debatepedia, to compare our QFAS approach with other models. The experimental results show that our approach outperforms the state-of-the-art models on three ROUGE scores.",
}

@misc{liu2019finetune,
      title={Fine-tune BERT for Extractive Summarization}, 
      author={Yang Liu},
      year={2019},
      eprint={1903.10318},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@inproceedings{lin-2004-rouge,
    title = "{ROUGE}: A Package for Automatic Evaluation of Summaries",
    author = "Lin, Chin-Yew",
    booktitle = "Text Summarization Branches Out",
    month = jul,
    year = "2004",
    address = "Barcelona, Spain",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W04-1013",
    pages = "74--81",
}
@article{2020,
   title={FEQA: A Question Answering Evaluation Framework for Faithfulness Assessment in Abstractive Summarization},
   url={http://dx.doi.org/10.18653/v1/2020.acl-main.454},
   DOI={10.18653/v1/2020.acl-main.454},
   journal={Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
   publisher={Association for Computational Linguistics},
   author={Durmus, Esin and He, He and Diab, Mona},
   year={2020}
}

@misc{maynez2020faithfulness,
      title={On Faithfulness and Factuality in Abstractive Summarization}, 
      author={Joshua Maynez and Shashi Narayan and Bernd Bohnet and Ryan McDonald},
      year={2020},
      eprint={2005.00661},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{Evaluating-the-Factual,
      title={Evaluating the Factual Consistency of Abstractive Text Summarization}, 
      author={Wojciech Kryściński and Bryan McCann and Caiming Xiong and Richard Socher},
      year={2019},
      eprint={1910.12840},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}





  
@inproceedings{Transformers,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  booktitle={Advances in neural information processing systems},
  pages={5998--6008},
  year={2017}
}


@inproceedings{salchner2022survey,
  title={A Survey of Automatic Text Summarization Using Graph Neural Networks},
  author={Salchner, Marco Ferdinand and Jatowt, Adam},
  booktitle={Proceedings of the 29th International Conference on Computational Linguistics},
  pages={6139--6150},
  year={2022}
}

@incollection{GNNBook-ch21-liu,
author = "Liu, Bang and Wu, Lingfei",
editor = "Wu, Lingfei and Cui, Peng and Pei, Jian and Zhao, Liang",
title = "Graph Neural Networks in Natural Language Processing",
booktitle = "Graph Neural Networks: Foundations, Frontiers, and Applications",
year = "2022",
publisher = "Springer Singapore",
address = "Singapore",
pages = "463--481",
}

@inproceedings{CNNDMdataset,
 author = {Hermann, Karl Moritz and Kocisky, Tomas and Grefenstette, Edward and Espeholt, Lasse and Kay, Will and Suleyman, Mustafa and Blunsom, Phil},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C. Cortes and N. Lawrence and D. Lee and M. Sugiyama and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Teaching Machines to Read and Comprehend},
 url = {https://proceedings.neurips.cc/paper_files/paper/2015/file/afdec7005cc9f14302cd0474fd0f3c96-Paper.pdf},
 volume = {28},
 year = {2015}
}

@incollection{GNNBook-ch4-tang,
author = "Tang, Jian and Liao, Renjie",
editor = "Wu, Lingfei and Cui, Peng and Pei, Jian and Zhao, Liang",
title = "Graph Neural Networks for Node Classification",
booktitle = "Graph Neural Networks: Foundations, Frontiers, and Applications",
year = "2022",
publisher = "Springer Singapore",
address = "Singapore",
pages = "41--61",
}

@misc{GAT,
      title={Graph Attention Networks}, 
      author={Petar Veličković and Guillem Cucurull and Arantxa Casanova and Adriana Romero and Pietro Liò and Yoshua Bengio},
      year={2018},
      eprint={1710.10903},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}
@misc{GCN,
      title={Semi-Supervised Classification with Graph Convolutional Networks}, 
      author={Thomas N. Kipf and Max Welling},
      year={2017},
      eprint={1609.02907},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{wang-etal-2020-heterogeneous,
    title = "Heterogeneous Graph Neural Networks for Extractive Document Summarization",
    author = "Wang, Danqing  and
      Liu, Pengfei  and
      Zheng, Yining  and
      Qiu, Xipeng  and
      Huang, Xuanjing",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.553",
    doi = "10.18653/v1/2020.acl-main.553",
    pages = "6209--6219",
}
